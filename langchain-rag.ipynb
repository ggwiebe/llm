{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m248.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /home/glenn/.local/lib/python3.10/site-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/glenn/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/glenn/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/glenn/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/glenn/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2022.6.15.1)\n",
      "Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: regex, tiktoken\n",
      "Successfully installed regex-2023.12.25 tiktoken-0.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (0.0.352)\n",
      "Requirement already satisfied: openai in /home/glenn/.local/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: chromadb in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (0.4.21)\n",
      "Requirement already satisfied: langchainhub in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (0.1.14)\n",
      "Requirement already satisfied: bs4 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (0.0.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/glenn/.local/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/glenn/.local/lib/python3.10/site-packages (from langchain) (1.4.41)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from langchain) (3.9.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.2 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from langchain) (0.0.6)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from langchain) (0.1.3)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.70 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from langchain) (0.0.75)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/glenn/.local/lib/python3.10/site-packages (from langchain) (1.23.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/glenn/.local/lib/python3.10/site-packages (from langchain) (1.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/glenn/.local/lib/python3.10/site-packages (from langchain) (2.28.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/glenn/.local/lib/python3.10/site-packages (from openai) (3.6.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/glenn/.local/lib/python3.10/site-packages (from openai) (0.26.0)\n",
      "Requirement already satisfied: sniffio in /home/glenn/.local/lib/python3.10/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/glenn/.local/lib/python3.10/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from chromadb) (0.108.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /home/glenn/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.18.3)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from chromadb) (3.1.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from chromadb) (3.3.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from chromadb) (1.16.3)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from chromadb) (0.43b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from chromadb) (1.22.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from chromadb) (0.15.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from chromadb) (6.1.1)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from chromadb) (1.60.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from chromadb) (4.1.2)\n",
      "Requirement already satisfied: typer>=0.9.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from chromadb) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from chromadb) (28.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from chromadb) (4.0.1)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from langchainhub) (2.31.0.6)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/glenn/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/glenn/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.33.0,>=0.29.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.32.0.post1)\n",
      "Requirement already satisfied: certifi in /home/glenn/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2022.6.15.1)\n",
      "Requirement already satisfied: httpcore==1.* in /home/glenn/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/glenn/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.13.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/glenn/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/glenn/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /home/glenn/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.11.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/glenn/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.4.1)\n",
      "Requirement already satisfied: requests-oauthlib in /home/glenn/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3<2.0,>=1.24.2 in /home/glenn/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.26.12)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1->langchain) (23.2)\n",
      "Requirement already satisfied: coloredlogs in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
      "Requirement already satisfied: protobuf in /home/glenn/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (4.21.5)\n",
      "Requirement already satisfied: sympy in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.11.0)\n",
      "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /home/glenn/.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.56.4)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.22.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.43b0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.43b0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.43b0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /home/glenn/.local/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (59.8.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.7.2)\n",
      "Requirement already satisfied: monotonic>=1.5 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/glenn/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.1.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/glenn/.local/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (1.1.3)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.20.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/glenn/.local/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.3)\n",
      "Requirement already satisfied: types-urllib3 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from types-requests<3.0.0.0,>=2.31.0.2->langchainhub) (1.26.25.14)\n",
      "Requirement already satisfied: httptools>=0.4.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/glenn/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/glenn/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.16.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/glenn/.local/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/glenn/.local/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in /home/glenn/.local/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.8.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.12.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/glenn/.local/lib/python3.10/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/glenn/.local/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/glenn/miniconda3/envs/llm/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain openai chromadb langchainhub bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42824"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "969"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[10].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\n",
    "    \"What are the approaches to Task Decomposition?\"\n",
    ")\n",
    "print(len(retrieved_docs))\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7f5a36a0f310>)\n",
      "| RunnableLambda(...), 'question': RunnablePassthrough()}\n"
     ]
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "context = {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rag_chain = (\n",
    "    # {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    context | prompt | llm | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through prompting techniques like Chain of Thought or Tree of Thoughts, which guide the model to think step by step and explore multiple reasoning possibilities. Task decomposition can also involve task-specific instructions or human inputs.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] template='Use the following pieces of context to answer the question at the end.\\nIf you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\nUse three sentences maximum and keep the answer as concise as possible.\\nAlways say \"thanks for asking!\" at the end of the answer.\\n{context}\\nQuestion: {question}\\nHelpful Answer:'\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "rag_prompt_custom = PromptTemplate.from_template(template)\n",
    "print(rag_prompt_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through prompting the model to think step by step or by providing task-specific instructions. Task decomposition helps in enhancing model performance and understanding the model's thinking process. Thanks for asking!\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt_custom\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the input documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'},\n",
       "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'},\n",
       "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'},\n",
       "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}],\n",
       " 'answer': 'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It allows agents or models to better understand and plan for the individual components of a task. Thanks for asking!'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | rag_prompt_custom\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"documents\": retriever, \"question\": RunnablePassthrough()}\n",
    ") | {\n",
    "    \"documents\": lambda input: [doc.metadata for doc in input[\"documents\"]],\n",
    "    \"answer\": rag_chain_from_docs,\n",
    "}\n",
    "\n",
    "rag_chain_with_source.invoke(\"What is Task Decomposition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "condense_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "condense_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "condense_q_chain = condense_q_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the definition of \"large\" in the context of a language model?'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "condense_q_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What does LLM stand for?\"),\n",
    "            AIMessage(content=\"Large language model\"),\n",
    "        ],\n",
    "        \"question\": \"What is meant by large\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do transformer models function?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condense_q_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What does LLM stand for?\"),\n",
    "            AIMessage(content=\"Large language model\"),\n",
    "        ],\n",
    "        \"question\": \"How do transformers work\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def condense_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return condense_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(context=condense_question | retriever | format_docs)\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Common ways of task decomposition include:\\n\\n1. Using Chain of Thought (CoT): CoT is a prompting technique where the model is instructed to \"think step by step\" and decompose complex tasks into smaller and simpler steps. This technique utilizes more test-time computation and helps the model understand the thinking process behind solving the task.\\n\\n2. Task-specific instructions: Providing task-specific instructions or prompts can guide the model in decomposing the task. For example, for writing a novel, the instruction \"Write a story outline\" can help break down the task into subgoals and steps.\\n\\n3. Human inputs: Task decomposition can also be done with the help of human inputs. Humans can provide guidance, expertise, or domain knowledge to assist in breaking down the task into manageable subtasks or steps.')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
